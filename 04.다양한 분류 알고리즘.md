로지스틱 회귀(이름은 회귀인데 분류임)

왜? 찾아보니 - 선형회귀 기반에 시그모이드 활성화함수로 변환해 해당 클래스일 확률 예측해서라고 함

**knn 분류의 출력값이 확률로 볼 수 없는 이유?** 

k개의 샘플 중에서의 빈도일 뿐 전체 데이터 분포에 대한 확률 모델이 아님

값이 불연속적임 (이산적인 값만 가능)

k값에 따라 달라짐 → 진짜 확률이라면 k와 무관해야 함

그냥 주변 표본 비율만 출력해 통계적으로 확률모델과는 다름

**그럼 k가 커지면 확률값 다워지지 않을까?**

k가 커지면 확률처럼 보이지만 **조건부 확률** 이 아니라 데이터셋 전체의 클래스 비율에 가까워짐 → 지역성이 줄어듬

로지스틱 회귀: 선형방정식 + 시그모이드

**시그모이드**: 하나의 선형 방정식의 출력값을 0~1값으로 압축. 작을수록 0에 수렴, 클수록 1에 수렴 - 이진분류에 

**소프트맥스**: 여러개의 선형(클래스별로 방정식이 있음) 방정식의 출력값을 0~1 사이로 압축하고 합이 1이 되도록 만듬 - 다중분류

---

데이터는 시간이 지날수록 늘어남. 

데이터가 바뀌면 기존 훈련한 모델을 버리고 다시 학습해야 할까? → 딱봐도 비효율적

늘어나면 이전 데이터를 버리고 데이터 크기를 유지해야 할까? → 이전 데이터에 중요한 정보가 있으면 치명적

→ 새로운 데이터에 대해서만 조금씩 더 훈련 - 점진적 학습, 온라인 학습

**확률적 경사 하강법**도 점진적 학습의 한 종류

**확률적 경사 하강법** : 전체 샘플을 사용하지 않고 랜덤으로 하나의 샘플을 사용해 조금씩 학습

랜덤으로 샘플을 선택하기 때문에 아주 조금씩 학습해야 함

**조금씩 학습하는 이유?**: 랜덤으로 샘플을 고르다보니 불완전해서 한번만에 최적의 방향을 잡지 못함. 조금씩 경사하강법을 수행해 여러번 반복하면서 전체 데이터의 최적의 해에 가까워질 수 있음 → 오히려 지역 최소성을 피할 수 있고 일반화 성능 좋아짐

**에포크**: 훈련 세트를 한 번 모두 사용하는 과정

**확률적** : 1개 샘플 사용해 경사하강법

장: 계산이 빠름, 로컬 미니멈에 갇히지 않고 탈출 가능

단: 수렴과정 불안정

**1개의 샘플을 정하는 방식 ?** - 

**랜덤**(몇개만 집중적으로 선택될 수 있음)

**셔플 후 순자척**으로 선택 (1번은 꼭 사용) - 많이 사용

**미니배치**: 여러개 샘플 사용해 경사하강법

장: 배치보다 빠르고, 확률적보다 안정적, 로컬 미니멈 탈출 유리

단: 배치 크기 설정이 성능에 영향 (너무 작으면 노이즈 증가, 크면 속도가 느림)

**가장 많이 사용하는 방식** - 배치사이즈만큼 샘플을 뽑아 평균 기울기로 업데이트

**배치**: 전체 샘플 사용해 경사하강법

장: 항상 최적해 방향으로 이동, 수 렴과정이 안정적

단: 연산량 큼, 시간 오래걸림

**손실함수**: 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준 - 작을수록 좋음 

어느정도 만족할 만한 값이면 받아들이는 편 (어디가 가장 최소인지 알 수 없기 때문에 타협)

분류 - **로지스틱 손실함수(이진 크로스엔트로피), 크로스 엔트로피**

회귀 - **mse**

**결론 : 데이터가 커짐에 따라 한번에 메모리에 올릴 수 없어서 조금씩 사용해 학습함.**
