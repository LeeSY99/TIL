**출력층** :  신경망의 최종 계산결과(은닉층까지의 연선)를 최종 예측값을 만드는 층

**뉴런** : 인공신경망에서 하나의 계산 단위

**입력층**: 데이터의 특징을 처음 받는 층. 자체적인 계산은 하지 않음

**딥러닝** -  여러개의 층을 가진 인공신경망

인공신경망은 교차검증을 잘 사용하지 않고 검증셋을 따로 둬서 사용 

왜?  1)  딥러닝 분야의 데이터셋은 충분히 큼.

1. 교차 검증을 수행하기엔 시간이 너무 오래 걸림

**밀집층(dense layer) = fully connected layer:** 양쪽 뉴런끼리 빠짐없이 연결되어 있음

**인공신경망에서 활성화 함수를 사용하는 이유**

모든 층이 선형변환만 있다면 여러층을 쌓아도 하나의 선형변환과 동일해져 버림 → **비선형관계 (복잡한 관계)를 학습할 수 없음**

**비선형성 부여, 특징 강조/억제, 출력범위 조정, 학습안정화(기울시 소실, 폭주 완화)**

**자주 사용하는 활성화 함수**

1. Sigmoid- 출력을 0~1로 조정, 확률 해석 가능 / 깊은 신경망에선 기울기 소실 문제
2. Tanh - 출력 -1~1, 중심이 0이라 학습 안정적 / 기울기 소실 문제
3. ReLU - 가장 많이 사용, 연산이 간단, 기울기 소실 완화
    1. **ReLU가 기울기 소실을 완화하는 원리**: 시그모이드와 달리 수렴하지 않음, 시그모이드의 미분은 최대 0.25 → 층이 깊어질수록 0에 가까워짐
4. Leaky ReLU - 음수 영역도 약간 전달
5. Softmax - 다중 분류에서 클래스별 확률로 변환

**손실 함수**

모델이 얼마나 잘못 예측했는지 수치로 표현하는 측정 지표

예측값과 실제값 차이를 계산에 얼마나 틀렸는지 계산

학습 방향 제시 → 손실 값을 최소화하는 방향으로

**손실함수 종류**

1. 회귀
    1. MSE(Mean Squared Error) → 오차 제곱 → 큰 오차에 큰 패널티
    2. MAE(Mean Absolute Error) → 절대값 기준 → 이상치에 덜 민감
2. 분류
    1. Binary Cross-Entropy(이진분류) - 시그모이드와 함께 씀
    2. Categorical Cross-Entropy (다중분류) → 정답 클래스의 확률이 높아지도록 학습(softmax)
3. 특수
    1. Hinge Loss: SVM
    2. Huber Loss: 회귀에서 이상치에 덜 민감하도록 MSE + MAE 절충
    3. KL Divergence : 두 확률 분포 차이 측정

**원-핫 인코딩** - 해당 클래스만 1이고 나머지는 0인 배열로 만드는 것

---

**은닉층** : 입력층과 출력층 사이에 모든 층

**은닉층에도 활성화 함수를 사용하는 이유**

단순히 가중합만 사용한다면 선형 변환의 합성일 뿐 → 한층짜리 선형모델과 다르지 않음

비선형성 확보

계층별로 특징 학습

1번째 은닉층 : 단순 패턴 등 학습

2번째 은닉층 : 더 복잡한 패턴 학습

… 깊은 층: 고차원 특징 학습

**옵티마이저**

손실 함수 최소화 - 손실함수를 줄이는 방향으로 파라미터 조정

효울적 학습 - 학습 속도를 바르게 하고 더 좋은 최소값에 도달하게 도움

안정성 - 안정적으로 수렴하도록 제어

→ 손실함수를 미분해서 경사가 낮아지는 방향으로 파라미터 조정

종류

1. 기본형
    1. Batch Gradient Decent: 전체 데이터로 한번에 업데이트 → 정확/ 느림
    2. Stochastic Gradient Decent : 샘플 1개씩 업데이트 → 빠름/ 불안정
    3. Mini-batch SGD: 데이터 일부로 업데이트 → 가장 많이 사용
2. 개선형
    1. Momentum: 이전 업데이트 방향을 고려해 진동 줄이고 빠르게 수렴
    2. RMSProp: 기울기의 제곱 평균으로 학습률 조정 → 안정적
    3. Adam (adaptive momdntum estimation) - momentum + rmsprop 결합 → 가장 많이 사용
    4. adagrad : 자주 등장하는 파라미터에는 작은 학습률, 드물게 등장하는 파라미터는 큰 학습률
    5. adadelta, nadam, adamW - adam의 개선

드롭아웃

일부 뉴런을 랜덤하게 꺼서 출력을 0으로 만듬 → 과대적합 막는 효과

**노드수를 줄이는 것과 드롭아웃을 하는 것의 차이**

노드 수를 줄여 모델을 작게 만듬

과적합 감소, 연산/메모리 사용 감소

그러나 표현력이 약해져 복잡한 문제는 잘 못 학습

드롭아웃

특정 뉴런/특징에 지나치게 의존하지 않게 함 → 앙상블 효과

**매번 다른 작은 신경망을 학습하는 효과**

테스트시에는 드롭아웃 적용 x
