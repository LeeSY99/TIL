# 8장 - 성능 최적화

1. **데이터**를 사용한 성능 최적화
    1. 최대한 많은 데이터 수집하기
    2. 데이터 생성하기 - 수집할 수 없다면 데이터를 만들어 사용 가능
    3. 데이터 범위(scale) 조정/정규화 규제화 표준화
2. **알고리즘**을 이용한 성능 최적화
3. **알고리즘 튜닝**을 위한 성능 최적화
    
    가장 많은 시간이 소요되는 부분. 모델을 하나 선택해 다양한 하이퍼파리마터를 훈련시키고 최적의 성능을 도출해야함
    
    하이퍼 파라미터들
    
    1. **진단** -  성능 향상이 어느순간 멈췄다면 원인을 분석해야함. 문제를 진단하는데 사용할 수 있는 것이 모델에 대한 평가
    2. **가중치** - 가중치의 초기값은 랜덤값. 애매하다면 비지도 학습을 이용해 사전훈련을 진행 후 학습 하는 방법도 있음
    3. **학습률** 
    4. **활성화 함수** - 바꿀 때 손실함수도 함께 변경하는 경우가 많음
    5. **배치/에포크**- 
    6. **옵티마이저/손실함수**
    7. **네트워크 구성(topology) -** 네트워크 층을 늘리고 뉴런개수를 줄이거나 하나의 은닉층에 뉴런을 여러개 포함시키거나 이런 방식이 있음
4. **앙상블**을 이용한 성능 최적화
    
    **앙상블** - 모델을 2개 이상 섞어서 사용하는 방식
    

### 2) 하드웨어를 이용한 성능 최적화

CPU - 명령어가 입력되는 순서대로 데이터를 처리하는 직렬 방식 → ALU(산술 논리 장치)가 적음

GPU - 병렬 처리를 위해 개발됨 → ALU 많음 → 연산 속도 빠름

역전파처럼 복잡한 미분연산은 병렬연산을 해야 빠름

CUDA(Computed Unified Device Architectur) 엔비디아 에서 개발한 GPU개발 툴 - 많은 양의 연산을 동시에 처리할 수 있음

### 3) 하이퍼파라미터를 이용한 성능 최적화

1. **배치 정규화**
    1. **정규화(normalization)** - 데이터 범위를 사용자가 원하는 범위로 제한 (ex. 이미지 데이터 픽셀 정보 0~255 값을 255로 나눠 0~1사이의 값을 갖게 함) - 특성 스케일링이라고도 함
    2. **규제화(regularization)** - 모델 복잡도를 줄이기 위해 제약을 두는 방법. 
        - **드롭아웃/조기종료**
    3. **표준화(standardization)** - 데이터를 평균 0, 표준편차 1 형태로 변환
    4. **배치 정규화(batch normalization)** 
        
        각 배치 별로 다양한 분포를 가진 데이터를 평균과 분산을 이용해 정규화
        
        **효과**
        
        수렴 가속 - 손실 곡선이 빨리 내려가고, 더 큰 학습률도 가능
        
        학습 안정화 - 기울기 소실/폭주를 완화
        
        가중치 초기화에 덜 민감
        
        **단점**
        
        배치 크기가 작으면 정규화가 안되거나 다른 방향으로 학습
        
        RNN에서는 계층별로 정규화 해야해서 더 복잡해짐 
        
2. **드롭 아웃**
    
    훈련할 때 일정 비율의 뉴런만 사용하고 나머지는 가중치 업데이트 x
    
    매 단계 (학습할 노드를) 랜덤으로 바꿔가면서 훈련
    
    노드를 임의로 끄면서 학습 → 꺼진 노드는 신호 전달  x → 과적합 방지
    
3. **조기 종료**
    
    매 에포크마다 검증 데이터에 대한 오차를 측정해 과적합이 발생하기 전에 멈춤
    
    언제 학습을 멈출지 정하는 거지 최고의 성능을 가진 모델을 보장하는건 아님