**imdb 데이터셋**

**토큰**: 소문자로 바꾸고 구둣점 삭제한 다음 공백으로 분리한 단어

**단어 인덱스**

0 : 패딩 토큰

1 : 시작 토큰 <START>

2: 사전에 없는 단어 <UNK>

3: 추가 예약 토큰 <UNUSED>

4~ 실제 단어 사전에 등록된 단어 매핑

**패딩** : 길이가 짧은 문장을 길이를 맞추기 위해 패딩토큰으로 채움

**케라스 패딩과 자를때 앞부분에 채우고 자름.**

**왜 앞부분에 채울까?**

rnn에선 앞 패딩을 많이쓰임. 초반에 나온 단어는 타임스탬프가 지날수록 희석되어 의미가 약해짐

또 후반부에 0이 많으면 단어 학습이 약해짐

의미있는 단어가 끝부분에 있다고 기대

트랜스포머는 뒤쪽 패딩을 선호

**왜 앞부분을 자를까?** 

리뷰데이터 같은 경우는 뒷부분에 감정이 드러나는 문장이 많음

요약같은게 목적이면 뒷부분을 자르는게 더 좋다

문장 길이를 맞췄다면 단어별로 원핫인코딩을 수행 (단어별 중요도가 모두 동일)

**보다 단어임베딩을 선호** → 메모리를 **효율적**으로 사용가능 (실제로 코랩에서 실행했더니 원핫인코딩은 램때문에 세션에 종료됐음..)

---

기본 순환층은 긴 시퀀스를 학습하기 어려움. (은닉 상태에 담긴 정보가 점점 희석되기 때문)

**LSTM**, **GRU** 가 등장

**LSTM (Long Short-Term Memory)** - 단기 기억을 오래 기억하기 위해

기본적인 개념은 동일. 계산하고 결과를 다음스텝에 재사용

**은닉상태** : 입력과 이전 타임스텝의 은닉상태를 가중치에 곱하고 활성화 함수 통과시켜 다음 은닉상태 만듬(시그모이드 사용) → tanh 함수를 통과한 값과 곱해져서 은닉상태를 만듬

**셀 상태** : 다음 층으로 전달되지 않고 LSTM셀에서 순환만 되는 값

입력과 은닉상태를 또 다른 가중

치 w_f에 곱한 다음 시그모이드 통과. 이전 타임스텝의 셀 상태와 곱해 새로운 셀 상태를 만듬 → tanh함수를 통과해 새 은닉상태 생성에 기여

입력과 은닉상태에 곱해지는 가중치 w_o, w_f가 다름

2개의 작은 셀이 더 추가되어 셀 상태를 만드는데 기여함 → 입력과 은닉상태를 다른 가중치에 곱함 → 하나는 sigmoid, 하나는 tanh에 통과해 이전 셀 상태와 더함

**삭제 게이트** -  셀 상태에 있는 정보 제거를 조절하는 역할

**입력 게이트** - 새 정보를 셀 상태에 추가를 조절하는 역할

**출력 게이트** - 이 셀 상태가 다음 은닉상태로 출력을 조절(어떤 정보를 내보낼지)

**순환층에 드롭아웃**

**dropout** - 셀의 입력에 드롭아웃 적용

**recurrent_dropout** - 순환되는 은닉상태에 드롭아웃

**GRU - Gated Recurrent Unit**

리셋 게이트 - 과거 정보를 얼마나 무시할지

업데이트 게이트 - 새 정보와 이전 정보를 어떻게 섞을지
