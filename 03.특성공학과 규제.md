## K-최근접 이웃 회귀 (KNN Regression)

- KNN 분류와 비슷하게, 예측 샘플에 가장 가까운 샘플 **k개를 골라 평균**을 내어 예측.

---

## 결정계수 (R²)

- 회귀 문제 성능 측정 도구 중 하나.
- 값이 **1에 가까울수록 성능이 좋음**.

---

## 과대적합 (Overfitting)

- 훈련 세트에 지나치게 학습하여, 새로운 데이터에서는 일반화 성능이 떨어지는 현상.

---

## KNN 회귀의 문제점

- 가까운 샘플의 평균을 내므로 **범위 밖 데이터 예측 불가능**
    
    (범위 밖 데이터가 들어오면 항상 같은 값 예측)
    
- *이상치(Outlier)**가 평균값을 크게 왜곡.
- 근처 데이터가 적을 경우, 소수 샘플이 결과를 크게 왜곡.

---

## KNN 분류의 문제점

- 차원이 커질수록 모든 점들이 비슷하게 멀어져 → **정확도 하락** (차원의 저주).
- 데이터가 불균형할 때, 소수 클래스는 무시되기 쉬움.
- 결정 경계가 불안정하여 경계 부근에서 **작은 변화에도 민감**.

---

## 선형회귀 (Linear Regression)

- 데이터를 **직선(선형 함수)**으로 학습.
- 직선은 특성을 잘 나타낼 수 있는 최적의 직선.

### 한계

- 현실 데이터는 **비선형인 경우가 많음**.
- 단순 직선 하나로는 표현력이 부족.

---

## 다항회귀 (Polynomial Regression)

- 다항식을 사용한 선형회귀.
- 항을 추가해 데이터에 더 잘 맞게 학습 가능.
- 비선형처럼 보이지만 여전히 **선형회귀의 한 형태**.

---

## 다중회귀 (Multiple Linear Regression)

- 여러 개의 독립변수(특성)를 사용하여 하나의 종속변수를 예측.

### 다항회귀 vs 다중회귀

- **다항회귀**: 독립변수 1개 → 거듭제곱 항 추가 (곡선 관계 모델링).
- **다중회귀**: 서로 다른 독립변수 여러 개를 사용.

---

## 독립변수와 종속변수

- **독립변수(Feature, Input)**: 원인·조건, 종속변수에 영향을 줌.
- **종속변수(Target, Output)**: 예측하고자 하는 목표값.

---

## 특성공학 (Feature Engineering)

- 기존 특성을 사용해 **새로운 특성**을 만드는 작업.
- 스케일 조정, 특성 생성, 중요한 특성 선택 등을 포함.

---

## 규제 (Regularization)

- **과대적합을 줄이는 방법** 중 하나.

### 종류

- **릿지(Ridge)**: 계수를 줄여 0에 가깝게 만듦.
- **라쏘(Lasso)**: 일부 계수를 아예 0으로 만들어 변수 선택 자동화.
